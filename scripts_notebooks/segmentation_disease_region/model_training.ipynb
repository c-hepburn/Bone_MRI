{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook provide the following:\n",
    "- path to the training dataset;\n",
    "- path to store the results (text files, containing model settings, loss function values and\n",
    "  average dice scores vs epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from datetime import date\n",
    "\n",
    "from db_load_pytable import LoadPyTable\n",
    "from model import UNet\n",
    "from augmentation import *\n",
    "from dice_generalization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide path to the training dataset;\n",
    "path_to_trainset = '/home/chepburn/MRI_Bone/Jupyter/ARTICLE_I/Databases/data_7-5-21'\n",
    "\n",
    "## Provide path to save trained model parameters;\n",
    "path_to_store = '/home/chepburn/MRI_Bone/Jupyter/ARTICLE_I/Results/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------- Specify hyper-parameters --------------------------------\n",
    "## ---------------- Model architecture;\n",
    "depth = 4\n",
    "filter_number = 4\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "## ---------------- Data augmentation;\n",
    "## Specify number of augmentation steps to perform within one epoch;\n",
    "steps = 350\n",
    "    \n",
    "## ---------------- Optimizer parameters;\n",
    "optimizer = 'Adam' \n",
    "batchsize = 4\n",
    "lr = 0.001 ## learning rate;\n",
    "epochs = 100\n",
    "\n",
    "## ---------------- Outputs;\n",
    "## Loss function value will be outputted every 10th epoch;\n",
    "output_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------- Specify transformation parameters (data augmentation) --------------------\n",
    "\n",
    "## Create Transform class object; pass a dictionary with transformation parameters;\n",
    "## Choose device_mode \"cuda\" to allocate the output on GPU;\n",
    "ranges = {}\n",
    "ranges['degree'] = 10\n",
    "ranges['scale_yx'] = (0.9,1.1)\n",
    "ranges['shear_yx'] = 0.2\n",
    "ranges['sigma_points'] = (3,12)\n",
    "\n",
    "transform = Transform()\n",
    "transform.warp_mode = 'reflect'\n",
    "transform.device_mode = 'cuda' \n",
    "transform.ranges = ranges\n",
    "\n",
    "## ----------------------------------- Evaluation metric ---------------------------------\n",
    "## Create Dice class object to compute average dice score at each epoch;\n",
    "dice = Dice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------- Save current settings to a text file --------------------------\n",
    "file = open(path_to_store + 'settings.txt', 'w')\n",
    "\n",
    "comments = '''\n",
    "'''\n",
    "settings_description = '''Comments: %s\n",
    "\n",
    "Model:\n",
    "    Model depth: %s;\n",
    "    Filter number: %s;\n",
    "    Convolutional kernel size: %s;\n",
    "    Padding: %s;\n",
    "\n",
    "Parameter Initialization: default initialization; (see pytorch documentation at\n",
    "//pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d);\n",
    "    \n",
    "Adam optimizer settings: \n",
    "    Batchsize: %s;\n",
    "    Learing rate: %s;\n",
    "    Epochs: %s;\n",
    "\n",
    "During each training epoch %s augmentation steps are performed;\n",
    "\n",
    "Data augmentation. Transformation parameters: \n",
    "ranges['degree'] = 10\n",
    "ranges['scale_yx'] = (0.9,1.1)\n",
    "ranges['shear_yx'] = 0.2\n",
    "ranges['sigma_points'] = (3,12)\n",
    "transform.warp_mode = 'reflect'\n",
    "\n",
    "Data is taken from: %s\n",
    "'''%(comments, depth, filter_number, kernel_size, padding, batchsize, lr, epochs, steps,\n",
    "     path_to_trainset)\n",
    "\n",
    "file.write(settings_description)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training dataset; pass pytable to torch dataloader;\n",
    "## Specify batchsize and whether to shuffle the data;\n",
    "trainset = LoadPyTable(path_to_trainset)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batchsize, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the model;\n",
    "model = UNet(depth = depth,\n",
    "             filter_number = filter_number,\n",
    "             kernel_size = kernel_size,\n",
    "             padding = padding)\n",
    "\n",
    "## Allocate model on GPU BEFORE constructing the optimizer;\n",
    "model = model.cuda() \n",
    "    \n",
    "## Construct optimizer (Adam);\n",
    "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "## Define loss function (BCE);\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "## Create lists to store loss function values, mean dice scores;\n",
    "trainloss, train_mean_scores = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ---------------------------------------------------------------------- Loop over epochs;\n",
    "## Note: model is in the training mode by default;\n",
    "for k in range(epochs):\n",
    "                    \n",
    "    ## Set an initial value of the loss funtion (to be accumulated over augmentation steps);\n",
    "    run_loss = 0\n",
    "    \n",
    "    ## Create a list to store dice score for each step, averaged over classes and samples in\n",
    "    ## the batch;\n",
    "    trainbatch_scores = []\n",
    "    \n",
    "    #### -------------------------------------------------------- Loop over augmentation steps; \n",
    "    for step in range(steps):\n",
    "\n",
    "        ## NB: Set gradients to zero not to accumulate them over the training epochs;\n",
    "        optim.zero_grad()\n",
    "\n",
    "        ## Create a list: [ [image batch 1, rs batch 1], ..., [image batch n, rs batch n] ];\n",
    "        ## (rs - reference standard);\n",
    "        datalist = list(trainloader)\n",
    "        \n",
    "        ## Generate a random integer from the range (0, number of batches);\n",
    "        rn = np.random.randint(low = 0, high = len(datalist))\n",
    "\n",
    "        ## Choose a random sample from the list: [image batch, rs batch];\n",
    "        ## Output: two 4D torch tensors of size [N,1,H,W]; (requires_grad = False);\n",
    "        ## (N - batch size);\n",
    "        sample = datalist[rn]\n",
    "        inputs, targets = sample[0], sample[1]\n",
    "\n",
    "        ## ---------------------------- Data Augmentation ------------------------------\n",
    "\n",
    "        ## Pass size of the input tensor to the Transform class object;\n",
    "        ## (size of the last batch may differ);\n",
    "        transform.dim = inputs.shape\n",
    "\n",
    "        ## Transform image, target batch (elastic, affine, random flip, intensity augmentation);;\n",
    "        ## Output: two 4D torch tensors of size [N,1,H,W], allocated on GPU;\n",
    "        t_inputs, t_targets = transform.batch_transform(inputs, targets)\n",
    "\n",
    "        ## -----------------------------------------------------------------------------\n",
    "\n",
    "        ## Pass transformed inputs to the model; \n",
    "        ## Output: 4D torch tensor [N,1,H,W];\n",
    "        ## (requires_grad = True => computational graph will be stored to compute gradients);\n",
    "        prediction = model(t_inputs)\n",
    "\n",
    "        ## Compute loss function value, given model prediction and transformed targets;\n",
    "        ## Output: 1D torch tensor;\n",
    "        loss = loss_fn(prediction, t_targets)\n",
    "\n",
    "        ## Increment run loss variable;\n",
    "        ## (use .item() method to retrieve float value from the output tensor);\n",
    "        run_loss += loss.item()\n",
    "\n",
    "        ## Compute the loss function gradients;\n",
    "        loss.backward()\n",
    "\n",
    "        ## Update model parameters;\n",
    "        optim.step()\n",
    "\n",
    "        ## ------------------------------- Compute dice score -----------------------------\n",
    "        ## Don't store the computational graph, unnecessary here;\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ## Inputs: two 4D torch tensors of size [N,1,H,W], output: float;\n",
    "            trainbatch_sd = dice.compute_average_dice(prediction, t_targets)\n",
    "            trainbatch_scores.append(trainbatch_sd) ## append to the list;\n",
    "        ## -------------------------------------------------------------------------------\n",
    "    #### -------------------------------------------------------------- Loop over steps ends;\n",
    "    \n",
    "    ## Save loss function value accumulated over steps;\n",
    "    trainloss.append(run_loss)\n",
    "    \n",
    "    ## Save dice scores, averaged over classes, samples in the batch and augmentation steps;\n",
    "    ## Convert list to numpy array to be able to compute mean;\n",
    "    trainbatch_scores = np.asarray(trainbatch_scores)\n",
    "    train_mean_scores.append( np.mean(trainbatch_scores) )\n",
    "\n",
    "    ## Output loss function value each XXth epoch; \n",
    "    if k % output_number == 0:\n",
    "\n",
    "        print('Loss at {}th iteration, training set: {}'.format(k,round(trainloss[k],2)))\n",
    "    \n",
    "    ## Save model parameters for the last 10 models;\n",
    "    if k > 90:\n",
    "        \n",
    "        ## Specify model name;\n",
    "        model_name = 'model_'+ str(date.today()) + '_epoch_'+ str(k) + '.pt'\n",
    "        torch.save( model.state_dict(), path_to_store + model_name)\n",
    "#### ----------------------------------------------------------------- Loop over epochs ends;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert lists to numpy arrays;\n",
    "trainloss = np.asarray(trainloss)\n",
    "train_mean_scores = np.asarray(train_mean_scores)\n",
    "\n",
    "## Save loss function value and average dice scores to a txt file;\n",
    "header = '''\n",
    "Col.0: Loss function value\n",
    "Col.1: Average 2D soft dice\n",
    "'''\n",
    "np.savetxt(path_to_store + 'training_details', np.c_[trainloss, train_mean_scores],\n",
    "           header = header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
