{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook provide the following:\n",
    "- path to the training dataset;\n",
    "- path to the validation dataset;\n",
    "- path to the model (model and optimizer state dictionaries), if training is resumed;\n",
    "- path to store the results (text files, containing model settings, loss function values and\n",
    "  average dice scores vs epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from datetime import date\n",
    "\n",
    "from db_load_pytable import LoadPyTable\n",
    "from model import UNet\n",
    "from augmentation import *\n",
    "from dice_generalization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide path to the training and validation datasets;\n",
    "path_to_trainset = ''\n",
    "path_to_valset = ''\n",
    "\n",
    "## Provide path to the existing model to resume training;\n",
    "path_to_model = None\n",
    "\n",
    "## Provide path to save trained model parameters;\n",
    "path_to_store = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------- Specify hyper-parameters --------------------------------\n",
    "## ---------------- Model architecture;\n",
    "depth = 4\n",
    "filter_number = 4\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "## ---------------- Data augmentation;\n",
    "## Specify number of augmentation steps to perform within one epoch;\n",
    "steps = 350\n",
    "    \n",
    "## ---------------- Parameter initialization; \n",
    "## Default: see pytorch documentation at\n",
    "## https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d ;\n",
    "## Otherwise, specify if training is resumed;\n",
    "resume_training = False\n",
    "strict = True \n",
    "\n",
    "## ---------------- Optimizer parameters;\n",
    "optimizer = 'Adam' \n",
    "batchsize = 4\n",
    "lr = 0.001 ## learning rate;\n",
    "epochs = 100\n",
    "\n",
    "## ---------------- Outputs;\n",
    "## Loss function value will be outputted every 10th epoch;\n",
    "output_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------- Specify transformation parameters (data augmentation) --------------------\n",
    "\n",
    "## Create Transform class object; pass a dictionary with transformation parameters;\n",
    "## Choose device_mode \"cuda\" to allocate the output on GPU;\n",
    "ranges = {}\n",
    "ranges['degree'] = 10\n",
    "ranges['scale_yx'] = (0.9,1.1)\n",
    "ranges['shear_yx'] = 0.2\n",
    "ranges['sigma_points'] = (3,12)\n",
    "\n",
    "transform = Transform()\n",
    "transform.warp_mode = 'reflect'\n",
    "transform.device_mode = 'cuda' \n",
    "transform.ranges = ranges\n",
    "\n",
    "## ----------------------------------- Evaluation metric ---------------------------------\n",
    "## Create Dice class object to compute average dice score at each epoch;\n",
    "dice = Dice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------------- Save current settings to a text file --------------------------\n",
    "file = open(path_to_store + 'settings.txt', 'w')\n",
    "\n",
    "comments = '''\n",
    "'''\n",
    "settings_description = '''Comments: %s\n",
    "\n",
    "Model:\n",
    "    Model depth: %s;\n",
    "    Filter number: %s;\n",
    "    Convolutional kernel size: %s;\n",
    "    Padding: %s;\n",
    "\n",
    "Parameter Initialization: default initialization;\n",
    "    Training is resumed: %s;\n",
    "    If resumed training, model is taken from: %s;\n",
    "    \n",
    "Adam optimizer settings: \n",
    "    Batchsize: %s;\n",
    "    Learing rate: %s;\n",
    "    Epochs: %s;\n",
    "\n",
    "During each training epoch %s augmentation steps are performed;\n",
    "\n",
    "Data augmentation. Transformation parameters: \n",
    "ranges['degree'] = 10\n",
    "ranges['scale_yx'] = (0.9,1.1)\n",
    "ranges['shear_yx'] = 0.2\n",
    "ranges['sigma_points'] = (3,12)\n",
    "transform.warp_mode = 'reflect'\n",
    "\n",
    "Data is taken from:\n",
    "    Training set: %s\n",
    "    Validation set: %s\n",
    "'''%(comments, depth, filter_number, kernel_size, padding, resume_training, path_to_model,\n",
    "     batchsize, lr, epochs, steps, path_to_trainset, path_to_valset)\n",
    "\n",
    "file.write(settings_description)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training, validation datasets; pass pytables to torch dataloader;\n",
    "## Specify batchsize and whether to shuffle the data;\n",
    "trainset = LoadPyTable(path_to_trainset)\n",
    "valset = LoadPyTable(path_to_valset)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batchsize, shuffle = True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------------- Model ----------------------------------------\n",
    "model = UNet(depth = depth,\n",
    "             filter_number = filter_number,\n",
    "             kernel_size = kernel_size,\n",
    "             padding = padding)\n",
    "\n",
    "model = model.cuda() ## Allocate model on GPU BEFORE constructing the optimizer;\n",
    "    \n",
    "## If training is resumed, load model state dictionary;\n",
    "if resume_training:\n",
    "    \n",
    "    dictionary = torch.load(path_to_model)\n",
    "    model.load_state_dict(dictionary['model_state_dict'], strict = strict)\n",
    "    \n",
    "    print('Model training is resumed. Model and optimizer parameters are taken from:')\n",
    "    print('\\t', path_to_model)\n",
    "\n",
    "## ---------------------------- Construct optimizer (Adam) ----------------------------------\n",
    "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "## If training is resumed, load optimizer state dictionary;\n",
    "## In case of transfer learning comment this part: load only model parameters via\n",
    "## model.load_state_dict(path, strict = ..);\n",
    "if resume_training:\n",
    "\n",
    "    optim.load_state_dict( dictionary['optimizer_state_dict'] )\n",
    "    print('Resuming training, loaded optimizer state dictionary.')\n",
    "\n",
    "## --------------------------- Define loss function (BCE) ----------------------------------\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "## -------------- Create lists to store loss function values, mean dice scores -------------\n",
    "trainloss, train_mean_scores = [], []\n",
    "valloss, val_mean_scores = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ---------------------------------------------------------------------- Loop over epochs;\n",
    "for k in range(epochs):\n",
    "                    \n",
    "    ## Set an initial value of the loss funtion (to be accumulated over augmentation steps);\n",
    "    run_loss = 0\n",
    "\n",
    "    ## NB: Pass model to the training mode; \n",
    "    model = model.train()\n",
    "    \n",
    "    ## Create a list to store dice score for each step, averaged over classes and samples in\n",
    "    ## the batch;\n",
    "    trainbatch_scores = []\n",
    "    \n",
    "    #### -------------------------------------------------------- Loop over augmentation steps;\n",
    "    for step in range(steps):\n",
    "\n",
    "        ## NB: Set gradients to zero not to accumulate them over the training epochs;\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        ## Create a list: [ [image batch 1, rs batch 1], ..., [image batch n, rs batch n] ];\n",
    "        ## (rs - reference standard);\n",
    "        datalist = list(trainloader)\n",
    "        \n",
    "        ## Generate a random integer from the range (0, number of batches);\n",
    "        rn = np.random.randint(low = 0, high = len(datalist))\n",
    "\n",
    "        ## Choose a random sample from the list: [image batch, rs batch];\n",
    "        ## Output: two 4D torch tensors of size [N,1,H,W]; (requires_grad = False);\n",
    "        ## (N - batch size);\n",
    "        sample = datalist[rn]\n",
    "        inputs, targets = sample[0], sample[1]\n",
    "\n",
    "        ## ---------------------------- Data Augmentation ------------------------------\n",
    "\n",
    "        ## Pass size of the input tensor to the Transform class object;\n",
    "        ## (size of the last batch may differ);\n",
    "        transform.dim = inputs.shape\n",
    "\n",
    "        ## Transform image, target batch (elastic, affine, random flip, intensity augmentation);;\n",
    "        ## Output: two 4D torch tensors of size [N,1,H,W], allocated on GPU;\n",
    "        t_inputs, t_targets = transform.batch_transform(inputs, targets)\n",
    "\n",
    "        ## -----------------------------------------------------------------------------\n",
    "\n",
    "        ## Pass transformed inputs to the model; \n",
    "        ## Output: 4D torch tensor [N,1,H,W];\n",
    "        ## (requires_grad = True => computational graph will be stored to compute gradients);\n",
    "        prediction = model(t_inputs)\n",
    "\n",
    "        ## Compute loss function value, given model prediction and transformed targets;\n",
    "        ## Output: 1D torch tensor;\n",
    "        loss = loss_fn(prediction, t_targets)\n",
    "\n",
    "        ## Increment run loss variable;\n",
    "        ## (use .item() method to retrieve float value from the output tensor);\n",
    "        run_loss += loss.item()\n",
    "\n",
    "        ## Compute the loss function gradients;\n",
    "        loss.backward()\n",
    "\n",
    "        ## Update model parameters;\n",
    "        optim.step()\n",
    "\n",
    "        ## ------------------------------- Compute dice score -----------------------------\n",
    "        ## Don't store the computational graph, unnecessary here;\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ## Inputs: two 4D torch tensors of size [N,1,H,W], output: float;\n",
    "            trainbatch_sd = dice.compute_average_dice(prediction, t_targets)\n",
    "            trainbatch_scores.append(trainbatch_sd) ## append to the list;\n",
    "\n",
    "        ## -------------------------------------------------------------------------------\n",
    "    #### -------------------------------------------------------------- Loop over steps ends;\n",
    "    \n",
    "    ## Save loss function value accumulated over steps;\n",
    "    trainloss.append(run_loss)\n",
    "    \n",
    "    ## Save dice scores, averaged over classes, samples in the batch and augmentation steps;\n",
    "    ## Convert list to numpy array to be able to compute mean;\n",
    "    trainbatch_scores = np.asarray(trainbatch_scores)\n",
    "    train_mean_scores.append( np.mean(trainbatch_scores) )\n",
    "\n",
    "    ## Re-initialize value of the loss function; (used in the next loop);\n",
    "    run_loss = 0\n",
    "    \n",
    "    ## NB: pass the model to evaluation mode;\n",
    "    model = model.eval()\n",
    "\n",
    "    ## Create a list to store dice score for each batch, averaged over classes and samples in\n",
    "    ## the batch;\n",
    "    valbatch_scores = []\n",
    "    \n",
    "    #### ------------------------------------------------------ Loop over validation batches;\n",
    "    for i, sample in enumerate(valloader):\n",
    "        \n",
    "        ## Retrieve image and reference standard batch;\n",
    "        ## Output: two 4D torch tensors of size [N,1,H,W]; (requires_grad = False);\n",
    "        inputs, targets = sample[0].cuda(), sample[1].cuda()\n",
    "        \n",
    "        ## Pass inputs to the model; don't store computational graph;\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            ## Output: 4D torch tensor of size [N,1,H,W]; (requires_grad = False);\n",
    "            prediction = model(inputs)\n",
    "        \n",
    "        ## Compute value of the loss function, i.e., after model parameters update;\n",
    "        loss = loss_fn(prediction, targets)\n",
    "\n",
    "        ## Increment run loss variable;\n",
    "        run_loss += loss.item()\n",
    "        \n",
    "        ## ------------------------------- Compute dice score -----------------------------\n",
    "        valbatch_sd = dice.compute_average_dice(prediction, targets)\n",
    "        valbatch_scores.append(valbatch_sd)     \n",
    "        ## --------------------------------------------------------------------------------\n",
    "    #### ----------------------------------------------- Loop over validation batches ends;\n",
    "    \n",
    "    ## Save loss function value, accumulated over batches;\n",
    "    valloss.append(run_loss)\n",
    "    \n",
    "    ## Save dice scores, averaged over classes, samples in the batch and number of batches;\n",
    "    ## Convert list to numpy array to be able to compute mean;\n",
    "    valbatch_scores = np.asarray(valbatch_scores)\n",
    "    val_mean_scores.append( np.mean(valbatch_scores) )\n",
    "    \n",
    "    ## Output loss function value each XXth epoch; \n",
    "    if k % output_number == 0:\n",
    "\n",
    "        print('Loss at {}th iteration, training set: {}'.format(k,round(trainloss[k],2)))\n",
    "        print('Loss at {}th iteration, validation set: {}\\n'.format(k,round(valloss[k],2)))\n",
    "\n",
    "    ## Save model and optimizer state dictionary;\n",
    "    ## Specify the condition on the epoch;\n",
    "    if k % 3 == 0:\n",
    "    \n",
    "        ## Specify model name;\n",
    "        model_name = 'model_'+ str(date.today()) + '_epoch_'+ str(k) + '.pt'\n",
    "\n",
    "        torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optim.state_dict(),\n",
    "                    'epoch': k,\n",
    "                    'trainloss': trainloss,\n",
    "                    'valloss': valloss,\n",
    "                    'train_scores': train_mean_scores,\n",
    "                    'val_scores': val_mean_scores\n",
    "                    }, path_to_store + model_name)       \n",
    "#### ----------------------------------------------------------------- Loop over epochs ends;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the final model parameters;\n",
    "model_name = 'model_'+ str(date.today()) + '_epoch_'+ str(k) + '.pt'\n",
    "torch.save( model.state_dict(), path_to_store + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert lists to numpy arrays;\n",
    "trainloss = np.asarray(trainloss)\n",
    "train_mean_scores = np.asarray(train_mean_scores)\n",
    "\n",
    "valloss = np.asarray(valloss)\n",
    "val_mean_scores = np.asarray(val_mean_scores)\n",
    "\n",
    "## Save loss function value and average dice scores to a txt file;\n",
    "header = '''\n",
    "Col.0: Training Loss\n",
    "Col.1: Validation Loss \n",
    "'''\n",
    "np.savetxt(path_to_store + 'losses', np.c_[trainloss, valloss], header = header)\n",
    "\n",
    "header = '''\n",
    "Col.0: Average 2D soft dice, training data\n",
    "Col.1: Average 2D soft dice, validation data\n",
    "'''\n",
    "np.savetxt(path_to_store + 'scores',np.c_[train_mean_scores, val_mean_scores],header = header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
